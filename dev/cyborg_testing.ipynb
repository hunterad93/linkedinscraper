{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows you to step through the various LinkedIN selenium functions intervening at whatever step is needed. I am not including the proxy related stuff here, because all of the protonVPN proxies are 'flagged' and could contribute to my accounts being flagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium_stealth import stealth\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_driver(agent=\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"):\n",
    "    try:\n",
    "        print('driver started')\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(agent)\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "        driver.maximize_window() #max size for consistency with element names\n",
    "        stealth(driver,\n",
    "            languages=[\"en-US\", \"en\"],\n",
    "            vendor=\"Google Inc.\",\n",
    "            platform=\"MacIntel\",\n",
    "            webgl_vendor=\"Apple Inc.\",\n",
    "            renderer=\"Apple GPU\",\n",
    "            fix_hairline=True,\n",
    "        )\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "def login_linkedin(driver, username, password):\n",
    "    # Navigate to LinkedIn, enter username and password, submit form\n",
    "    # If verification page appears, call handle_verification\n",
    "    # Navigate to the LinkedIn login page\n",
    "    driver.get('https://www.linkedin.com/login')\n",
    "    \n",
    "    # Input username\n",
    "    username_field = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.ID, 'username')))\n",
    "    username_field.send_keys(username)\n",
    "    sleep(random.random()*3)    \n",
    "\n",
    "\n",
    "    # Input password\n",
    "    password_field = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.ID, 'password')))\n",
    "    password_field.send_keys(password)\n",
    "    sleep(random.random()*3)    \n",
    "\n",
    "    # Locate the sign in button\n",
    "    sign_in_button = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"submit\"]')))\n",
    "\n",
    "    # Create an ActionChains object mouse movement to avoid detection\n",
    "    actions = ActionChains(driver)\n",
    "\n",
    "    # Move to the sign in button and click it\n",
    "    actions.move_to_element(sign_in_button).click().perform()\n",
    "\n",
    "\n",
    "\n",
    "def get_verification_code_from_file(download_dir='/Users/adamhunter/Downloads'):\n",
    "    # Get a list of all the pgp*.txt files in the download directory\n",
    "    files = glob.glob(os.path.join(download_dir, 'pgp*.txt'))\n",
    "\n",
    "    # Find the most recent file\n",
    "    latest_file = max(files, key=os.path.getctime)\n",
    "\n",
    "    # Open the file and read the contents\n",
    "    with open(latest_file, 'r') as f:\n",
    "        contents = f.read()\n",
    "\n",
    "    # Use regex to find the verification code in the line that starts with \"Subject:\"\n",
    "    match = re.search(r'Subject:.*?(\\d{6})', contents)\n",
    "    if match:\n",
    "        verification_code = match.group(1)\n",
    "    else:\n",
    "        print(\"No verification code found in email.\")\n",
    "        verification_code = None\n",
    "\n",
    "    return verification_code\n",
    "\n",
    "def grab_verification(driver, username, password):\n",
    "    # Save the handle of the original tab\n",
    "    original_tab = driver.current_window_handle\n",
    "\n",
    "    # Open a new tab\n",
    "    driver.execute_script(\"window.open('');\")\n",
    "\n",
    "    # Switch to the new tab (it's always the last one)\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "    # Navigate to ProtonMail\n",
    "    driver.get('https://mail.protonmail.com/login')\n",
    "    # Input username\n",
    "    username_field = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.ID, 'username')))\n",
    "    sleep(random.random()*2)    \n",
    "    username_field.send_keys(username)\n",
    "\n",
    "    password_field = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.ID, 'password')))\n",
    "    sleep(random.random()*3)    \n",
    "    password_field.send_keys(password)\n",
    "\n",
    "    # Submit form\n",
    "    login_button = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.button-large')))\n",
    "    sleep(random.random()*3)    \n",
    "    login_button.click()\n",
    "    print('clicked login to email')\n",
    "\n",
    "    # Click the first email in the inbox\n",
    "    first_email = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.item-container-wrapper:nth-child(1) .item-subject .max-w100')))\n",
    "    first_email.click()\n",
    "\n",
    "    # Click 'More options' button\n",
    "    more_options_button = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.button:nth-child(11)')))\n",
    "    more_options_button.click()\n",
    "\n",
    "    # Click 'View headers' button\n",
    "    view_headers_button = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.dropdown-item:nth-child(10) .flex-item-fluid')))\n",
    "    view_headers_button.click()\n",
    "\n",
    "    # Click 'Download' button\n",
    "    download_button = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.button-solid-norm:nth-child(2)')))\n",
    "    download_button.click()\n",
    "\n",
    "    \n",
    "\n",
    "    # Switch back to the original tab\n",
    "    driver.switch_to.window(original_tab)\n",
    "\n",
    "    sleep(3)\n",
    "\n",
    "    verification_code = get_verification_code_from_file()\n",
    "\n",
    "    # Input verification code\n",
    "    verification_field = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.ID, 'input__email_verification_pin')))\n",
    "    sleep(random.random()*3)    \n",
    "    verification_field.send_keys(verification_code)\n",
    "    sleep(random.random())\n",
    "\n",
    "    # Click the verify button\n",
    "    verify_button = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.ID, 'email-pin-submit-button')))\n",
    "\n",
    "    # Create an ActionChains object mouse movement to avoid detection\n",
    "    actions = ActionChains(driver)\n",
    "\n",
    "    # Move to the sign in button and click it\n",
    "    actions.move_to_element(verify_button).click().perform()\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "def collect_group_or_network_links(driver, scraped_profiles, target_count=10):\n",
    "    \n",
    "    collected_links = set()\n",
    "\n",
    "    while len(collected_links) < target_count:\n",
    "        print(len(collected_links))\n",
    "\n",
    "        # Scroll to the top and wait 1-3 seconds triggering infinite scroll basically\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        sleep(1 + 2*random.random())\n",
    "\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        sleep(1 + 2*random.random())\n",
    "        \n",
    "        # Collect all links\n",
    "        all_links = WebDriverWait(driver, 30).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"a\")))\n",
    "        member_links = [link.get_attribute('href') for link in all_links if link.get_attribute('href').startswith(\"https://www.linkedin.com/in/\")]\n",
    "\n",
    "        # Use regex to shorten the member links to cutoff anything after the profile slug\n",
    "        member_links = [re.match(\"(https://www.linkedin.com/in/[^/]*)\", link).group(1) for link in member_links]\n",
    "\n",
    "        # Convert the list to a set to remove duplicates\n",
    "        member_links = set(member_links)\n",
    "\n",
    "        # Filter the member_links to include only those that are not in the scraped_profiles\n",
    "        new_links = [link for link in member_links if link not in scraped_profiles]\n",
    "\n",
    "        # Add the new links to the collected links set\n",
    "        collected_links.update(new_links)\n",
    "\n",
    "    # Switch to the current tab\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "    # Convert the set back to a list and return it\n",
    "    return list(collected_links)\n",
    "\n",
    "def collect_search_links(driver, scraped_profiles,page_number=2, target_count=100):\n",
    "    # Initialize an empty set to store the collected links\n",
    "    collected_links = set()\n",
    "\n",
    "    # Try to read the page number from a file\n",
    "    with open('../reference/page_number.txt', 'r') as f:\n",
    "        page_number = int(f.read())\n",
    "    print(page_number)\n",
    "    while len(collected_links) < target_count:\n",
    "        # Define the search URL\n",
    "        search_url = f\"https://www.linkedin.com/search/results/PEOPLE/?geoUrn=%5B%22103644278%22%5D&keywords=data%20analyst&network=%5B%22F%22%2C%22S%22%2C%22O%22%5D&origin=FACETED_SEARCH&page={page_number}&sid=m0%3A\"\n",
    "\n",
    "        # Navigate to the search URL\n",
    "        driver.get(search_url)\n",
    "\n",
    "        # Wait for the page to load and collect all links\n",
    "        all_links = WebDriverWait(driver, 30).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"a\")))\n",
    "        member_links = [link.get_attribute('href') for link in all_links if link.get_attribute('href').startswith(\"https://www.linkedin.com/in/\")]\n",
    "\n",
    "        # Use regex to shorten the member links to cutoff anything after the profile slug\n",
    "        member_links = [re.match(\"(https://www.linkedin.com/in/[^/]*)\", link).group(1) for link in member_links]\n",
    "\n",
    "        # Convert the list to a set to remove duplicates\n",
    "        member_links = set(member_links)\n",
    "\n",
    "        # Filter the member_links to include only those that are not in the scraped_profiles\n",
    "        new_links = [link for link in member_links if link not in scraped_profiles]\n",
    "\n",
    "        # Add the member links to the collected links set\n",
    "        collected_links.update(new_links)\n",
    "\n",
    "        # Increment the page number\n",
    "        page_number += 1\n",
    "\n",
    "        # Save the current page number to a file\n",
    "        with open('page_number.txt', 'w') as f:\n",
    "            f.write(str(page_number))\n",
    "        \n",
    "        sleep(10+20*random.random())\n",
    "\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "        # Convert the set back to a list and return it\n",
    "    return list(collected_links)\n",
    "\n",
    "\n",
    "def scrape_all(driver, member_links, num_to_scrape=None):\n",
    "    # If num_to_scrape is not specified, scrape all member_links\n",
    "    if num_to_scrape is None:\n",
    "        num_to_scrape = len(member_links)\n",
    "\n",
    "    for member_link in member_links[:num_to_scrape]:\n",
    "        try:\n",
    "            driver.get(member_link)\n",
    "            print('scraping'+member_link)\n",
    "            sleep(10+20*random.random())\n",
    "            # Get the page source and save it as a .txt file\n",
    "            page_source = driver.page_source\n",
    "            # Extract the profile name from the member link\n",
    "            profile_name = member_link.rstrip('/').split('/')[-1]            # Use the profile name to name the .txt file\n",
    "            with open(f'../data/{profile_name}_page_source.txt', 'w') as f:\n",
    "                f.write(page_source)\n",
    "            # Open the scraped_profiles file in append mode\n",
    "            with open('../reference/scraped_profiles.txt', 'a') as f:\n",
    "                # Write the member_link to the file\n",
    "                f.write(member_link + '\\n')\n",
    "            \n",
    "            # Check if 'sign in' is in the page title\n",
    "            if 'sign in' in driver.title.lower():\n",
    "                # If 'sign in' is in the title, abort the loop\n",
    "                break\n",
    "\n",
    "\n",
    "            sleep(1+3*random.random())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    # Switch to the current tab\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver started\n"
     ]
    }
   ],
   "source": [
    "driver = create_driver()\n",
    "driver.get('https://www.linkedin.com/login')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt auto login, or manually login and skip the next cell. Add your username and password here. This account should be a real trusted account, to be used for pulling links from a application-acceptance based LinkedIN group. If you want to use a all-accepting linkedin group then just use a bot account for this part too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "trusted_password = os.environ.get('ACTUAL_LINKED_IN_PASS')\n",
    "trusted_username = os.environ.get('ACTUAL_LINKED_IN_ACC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_linkedin(driver, trusted_username, trusted_password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the opened tab to navigate to group page or 'my network' page collect_links will gather profile urls up to a chosen limit, checking against list of profiles already scraped. You could probably set this amount pretty high, like 1,000, and maybe not get in trouble since all that will happen from linkedIn's perspective is a login followed by scrolling down a huge list in a group. This step requires the driver to be in focus, possibly because it is executing js commands to scroll to top and to bottom. This doesn't seem like a huge issue in general since the links gathered in this step take 20x longer to actually scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../reference/scraped_profiles.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      2\u001b[0m     scraped_profiles \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m collected_links \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_search_links\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscraped_profiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../reference/links_to_scrape.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m collected_links:\n",
      "Cell \u001b[0;32mIn[5], line 188\u001b[0m, in \u001b[0;36mcollect_search_links\u001b[0;34m(driver, scraped_profiles, page_number, target_count)\u001b[0m\n\u001b[1;32m    185\u001b[0m search_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.linkedin.com/search/results/PEOPLE/?geoUrn=%5B%22103644278%22%5D&keywords=data%20analyst&network=%5B%22F%22%2C%22S%22%2C%22O%22%5D&origin=FACETED_SEARCH&page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&sid=m0%3A\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Navigate to the search URL\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Wait for the page to load and collect all links\u001b[39;00m\n\u001b[1;32m    191\u001b[0m all_links \u001b[38;5;241m=\u001b[39m WebDriverWait(driver, \u001b[38;5;241m30\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39mpresence_of_all_elements_located((By\u001b[38;5;241m.\u001b[39mTAG_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "File \u001b[0;32m~/miniconda3/envs/craigslist-env/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:353\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/craigslist-env/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:342\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    340\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m--> 342\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/craigslist-env/lib/python3.9/site-packages/selenium/webdriver/remote/remote_connection.py:297\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    295\u001b[0m data \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdump_json(params)\n\u001b[1;32m    296\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/craigslist-env/lib/python3.9/site-packages/selenium/webdriver/remote/remote_connection.py:318\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    315\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 318\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/craigslist-env/lib/python3.9/site-packages/urllib3/request.py:81\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m     78\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m     79\u001b[0m     )\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/craigslist-env/lib/python3.9/site-packages/urllib3/request.py:173\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    170\u001b[0m extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(headers)\n\u001b[1;32m    171\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/craigslist-env/lib/python3.9/site-packages/urllib3/poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/miniconda3/envs/craigslist-env/lib/python3.9/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/craigslist-env/lib/python3.9/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/craigslist-env/lib/python3.9/site-packages/urllib3/connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/craigslist-env/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/craigslist-env/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/craigslist-env/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/craigslist-env/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('../reference/scraped_profiles.txt', 'r') as file:\n",
    "    scraped_profiles = file.read().split('\\n')\n",
    "\n",
    "collected_links = collect_search_links(driver,scraped_profiles,target_count = 30)\n",
    "\n",
    "with open('../reference/links_to_scrape.txt', 'a') as file:\n",
    "    for link in collected_links:\n",
    "        file.write(\"%s\\n\" % link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only quit here if you are going to a different account for the profile scraping phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with a list of profiles accounts and user-agent strings, pick a random account and do some scraping. In a fully fledged version this would iterate through all of them rather than pick a random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "accounts_df = pd.read_csv('../reference/accounts.csv')\n",
    "\n",
    "# Filter the DataFrame to include only active accounts\n",
    "active_accounts = accounts_df[accounts_df['acc_status'] == 1]\n",
    "\n",
    "# Select a random row from the DataFrame\n",
    "random_account = active_accounts.sample(1).iloc[0]\n",
    "\n",
    "bot_username = random_account['username']\n",
    "user_agent = random_account['user_agent']\n",
    "bot_password = random_account['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver started\n"
     ]
    }
   ],
   "source": [
    "driver = create_driver(user_agent)\n",
    "login_linkedin(driver, bot_username, bot_password)\n",
    "restriction_text = '</h1><p>We\\'ve restricted your account until '\n",
    "if restriction_text in driver.page_source:\n",
    "    print(driver.page_source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will work if the bot accounts are created with protonmail, using the same password as their associated linkedin acc. At some point I should probably generate a random list of human sounding emails and passwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Let's do a quick verification\" in driver.page_source:\n",
    "    grab_verification(driver, bot_username, bot_password)\n",
    "    # Get the verification code from the most recent pgp*.txt file\n",
    "elif \"Let's do a quick security check\" in driver.page_source: #captcha page\n",
    "    driver.switch_to.window(driver.current_window_handle) #brings page to focus for you\n",
    "    sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a number in scrape all to limit number of links scraped or leave blank to scrape all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1337\n"
     ]
    }
   ],
   "source": [
    "with open('../reference/scraped_profiles.txt', 'r') as file:\n",
    "    scraped_profiles = file.read().split('\\n')\n",
    "\n",
    "with open('../reference/links_to_scrape.txt', 'r') as file:\n",
    "    links_to_scrape = file.read().split('\\n')\n",
    "    \n",
    "links_to_scrape = list(set(links_to_scrape)) # Remove duplicates by converting to set and back to list\n",
    "\n",
    "# Remove any links that are already in scraped_profiles\n",
    "links_to_scrape = [link for link in links_to_scrape if link not in scraped_profiles]\n",
    "\n",
    "print(len(links_to_scrape))\n",
    "\n",
    "with open('../reference/links_to_scrape.txt', 'w') as file:\n",
    "    for link in links_to_scrape:\n",
    "        file.write(\"%s\\n\" % link)\n",
    "\n",
    "with open('../reference/links_to_scrape.txt', 'r') as file:\n",
    "    collected_links = file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrapinghttps://www.linkedin.com/in/mathias-jaeger-3776b1107/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mscrape_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollected_links\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 230\u001b[0m, in \u001b[0;36mscrape_all\u001b[0;34m(driver, member_links, num_to_scrape)\u001b[0m\n\u001b[1;32m    228\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(member_link)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscraping\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mmember_link)\n\u001b[0;32m--> 230\u001b[0m \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Get the page source and save it as a .txt file\u001b[39;00m\n\u001b[1;32m    232\u001b[0m page_source \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scrape_all(driver, collected_links,200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "craigslist-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
