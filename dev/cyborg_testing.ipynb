{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows you to step through the various LinkedIN selenium functions intervening at whatever step is needed. I am not including the proxy related stuff here, because all of the protonVPN proxies are 'flagged' and could contribute to my accounts being flagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium_stealth import stealth\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_driver(agent=\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"):\n",
    "    try:\n",
    "        print('driver started')\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(agent)\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "        driver.maximize_window() #max size for consistency with element names\n",
    "        stealth(driver,\n",
    "            languages=[\"en-US\", \"en\"],\n",
    "            vendor=\"Google Inc.\",\n",
    "            platform=\"MacIntel\",\n",
    "            webgl_vendor=\"Apple Inc.\",\n",
    "            renderer=\"Apple GPU\",\n",
    "            fix_hairline=True,\n",
    "        )\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "def login_linkedin(driver, username, password):\n",
    "    # Navigate to LinkedIn, enter username and password, submit form\n",
    "    # If verification page appears, call handle_verification\n",
    "    # Navigate to the LinkedIn login page\n",
    "    driver.get('https://www.linkedin.com/login')\n",
    "    \n",
    "    # Input username\n",
    "    username_field = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.ID, 'username')))\n",
    "    username_field.send_keys(username)\n",
    "    sleep(random.random()*3)    \n",
    "\n",
    "\n",
    "    # Input password\n",
    "    password_field = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.ID, 'password')))\n",
    "    password_field.send_keys(password)\n",
    "    sleep(random.random()*3)    \n",
    "\n",
    "    # Locate the sign in button\n",
    "    sign_in_button = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"submit\"]')))\n",
    "\n",
    "    # Create an ActionChains object mouse movement to avoid detection\n",
    "    actions = ActionChains(driver)\n",
    "\n",
    "    # Move to the sign in button and click it\n",
    "    actions.move_to_element(sign_in_button).click().perform()\n",
    "\n",
    "\n",
    "\n",
    "def get_verification_code_from_file(download_dir='/Users/adamhunter/Downloads'):\n",
    "    # Get a list of all the pgp*.txt files in the download directory\n",
    "    files = glob.glob(os.path.join(download_dir, 'pgp*.txt'))\n",
    "\n",
    "    # Find the most recent file\n",
    "    latest_file = max(files, key=os.path.getctime)\n",
    "\n",
    "    # Open the file and read the contents\n",
    "    with open(latest_file, 'r') as f:\n",
    "        contents = f.read()\n",
    "\n",
    "    # Use regex to find the verification code in the line that starts with \"Subject:\"\n",
    "    match = re.search(r'Subject:.*?(\\d{6})', contents)\n",
    "    if match:\n",
    "        verification_code = match.group(1)\n",
    "    else:\n",
    "        print(\"No verification code found in email.\")\n",
    "        verification_code = None\n",
    "\n",
    "    return verification_code\n",
    "\n",
    "def grab_verification(driver, username, password):\n",
    "    # Save the handle of the original tab\n",
    "    original_tab = driver.current_window_handle\n",
    "\n",
    "    # Open a new tab\n",
    "    driver.execute_script(\"window.open('');\")\n",
    "\n",
    "    # Switch to the new tab (it's always the last one)\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "    # Navigate to ProtonMail\n",
    "    driver.get('https://mail.protonmail.com/login')\n",
    "    # Input username\n",
    "    username_field = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.ID, 'username')))\n",
    "    sleep(random.random()*2)    \n",
    "    username_field.send_keys(username)\n",
    "\n",
    "    password_field = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.ID, 'password')))\n",
    "    sleep(random.random()*3)    \n",
    "    password_field.send_keys(password)\n",
    "\n",
    "    # Submit form\n",
    "    login_button = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.button-large')))\n",
    "    sleep(random.random()*3)    \n",
    "    login_button.click()\n",
    "    print('clicked login to email')\n",
    "\n",
    "    # Click the first email in the inbox\n",
    "    first_email = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.item-container-wrapper:nth-child(1) .item-subject .max-w100')))\n",
    "    first_email.click()\n",
    "\n",
    "    # Click 'More options' button\n",
    "    more_options_button = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.button:nth-child(11)')))\n",
    "    more_options_button.click()\n",
    "\n",
    "    # Click 'View headers' button\n",
    "    view_headers_button = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.dropdown-item:nth-child(10) .flex-item-fluid')))\n",
    "    view_headers_button.click()\n",
    "\n",
    "    # Click 'Download' button\n",
    "    download_button = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.button-solid-norm:nth-child(2)')))\n",
    "    download_button.click()\n",
    "\n",
    "    \n",
    "\n",
    "    # Switch back to the original tab\n",
    "    driver.switch_to.window(original_tab)\n",
    "\n",
    "    sleep(3)\n",
    "\n",
    "    verification_code = get_verification_code_from_file()\n",
    "\n",
    "    # Input verification code\n",
    "    verification_field = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.ID, 'input__email_verification_pin')))\n",
    "    sleep(random.random()*3)    \n",
    "    verification_field.send_keys(verification_code)\n",
    "    sleep(random.random())\n",
    "\n",
    "    # Click the verify button\n",
    "    verify_button = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.ID, 'email-pin-submit-button')))\n",
    "\n",
    "    # Create an ActionChains object mouse movement to avoid detection\n",
    "    actions = ActionChains(driver)\n",
    "\n",
    "    # Move to the sign in button and click it\n",
    "    actions.move_to_element(verify_button).click().perform()\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "def collect_group_or_network_links(driver, scraped_profiles, target_count=10):\n",
    "    \n",
    "    collected_links = set()\n",
    "\n",
    "    while len(collected_links) < target_count:\n",
    "        print(len(collected_links))\n",
    "\n",
    "        # Scroll to the top and wait 1-3 seconds triggering infinite scroll basically\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        sleep(1 + 2*random.random())\n",
    "\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        sleep(1 + 2*random.random())\n",
    "        \n",
    "        # Collect all links\n",
    "        all_links = WebDriverWait(driver, 30).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"a\")))\n",
    "        member_links = [link.get_attribute('href') for link in all_links if link.get_attribute('href').startswith(\"https://www.linkedin.com/in/\")]\n",
    "\n",
    "        # Use regex to shorten the member links to cutoff anything after the profile slug\n",
    "        member_links = [re.match(\"(https://www.linkedin.com/in/[^/]*)\", link).group(1) for link in member_links]\n",
    "\n",
    "        # Convert the list to a set to remove duplicates\n",
    "        member_links = set(member_links)\n",
    "\n",
    "        # Filter the member_links to include only those that are not in the scraped_profiles\n",
    "        new_links = [link for link in member_links if link not in scraped_profiles]\n",
    "\n",
    "        # Add the new links to the collected links set\n",
    "        collected_links.update(new_links)\n",
    "\n",
    "    # Switch to the current tab\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "    # Convert the set back to a list and return it\n",
    "    return list(collected_links)\n",
    "\n",
    "def collect_search_links(driver, scraped_profiles,page_number=2, target_count=100):\n",
    "    # Initialize an empty set to store the collected links\n",
    "    collected_links = set()\n",
    "\n",
    "    # Try to read the page number from a file\n",
    "    with open('../reference/page_number.txt', 'r') as f:\n",
    "        page_number = int(f.read())\n",
    "    print(page_number)\n",
    "    while len(collected_links) < target_count:\n",
    "        # Define the search URL\n",
    "        search_url = f\"https://www.linkedin.com/search/results/PEOPLE/?geoUrn=%5B%22103644278%22%5D&keywords=data%20analyst&network=%5B%22F%22%2C%22S%22%2C%22O%22%5D&origin=FACETED_SEARCH&page={page_number}&sid=m0%3A\"\n",
    "\n",
    "        # Navigate to the search URL\n",
    "        driver.get(search_url)\n",
    "\n",
    "        # Wait for the page to load and collect all links\n",
    "        all_links = WebDriverWait(driver, 30).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"a\")))\n",
    "        member_links = [link.get_attribute('href') for link in all_links if link.get_attribute('href').startswith(\"https://www.linkedin.com/in/\")]\n",
    "\n",
    "        # Use regex to shorten the member links to cutoff anything after the profile slug\n",
    "        member_links = [re.match(\"(https://www.linkedin.com/in/[^/]*)\", link).group(1) for link in member_links]\n",
    "\n",
    "        # Convert the list to a set to remove duplicates\n",
    "        member_links = set(member_links)\n",
    "\n",
    "        # Filter the member_links to include only those that are not in the scraped_profiles\n",
    "        new_links = [link for link in member_links if link not in scraped_profiles]\n",
    "\n",
    "        # Add the member links to the collected links set\n",
    "        collected_links.update(new_links)\n",
    "\n",
    "        # Increment the page number\n",
    "        page_number += 1\n",
    "\n",
    "        # Save the current page number to a file\n",
    "        with open('page_number.txt', 'w') as f:\n",
    "            f.write(str(page_number))\n",
    "        \n",
    "        sleep(10+20*random.random())\n",
    "\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "        # Convert the set back to a list and return it\n",
    "    return list(collected_links)\n",
    "\n",
    "\n",
    "def scrape_all(driver, member_links, num_to_scrape=None):\n",
    "    # If num_to_scrape is not specified, scrape all member_links\n",
    "    if num_to_scrape is None:\n",
    "        num_to_scrape = len(member_links)\n",
    "\n",
    "    for member_link in member_links[:num_to_scrape]:\n",
    "        try:\n",
    "            driver.get(member_link)\n",
    "            print('scraping'+member_link)\n",
    "            sleep(10+20*random.random())\n",
    "            # Get the page source and save it as a .txt file\n",
    "            page_source = driver.page_source\n",
    "            # Extract the profile name from the member link\n",
    "            profile_name = member_link.rstrip('/').split('/')[-1]            # Use the profile name to name the .txt file\n",
    "            with open(f'../data/{profile_name}_page_source.txt', 'w') as f:\n",
    "                f.write(page_source)\n",
    "            # Open the scraped_profiles file in append mode\n",
    "            with open('../reference/scraped_profiles.txt', 'a') as f:\n",
    "                # Write the member_link to the file\n",
    "                f.write(member_link + '\\n')\n",
    "            \n",
    "            # Check if 'sign in' is in the page title\n",
    "            if 'sign in' in driver.title.lower():\n",
    "                # If 'sign in' is in the title, abort the loop\n",
    "                break\n",
    "\n",
    "\n",
    "            sleep(1+3*random.random())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    # Switch to the current tab\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = create_driver()\n",
    "driver.get('https://www.linkedin.com/login')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt auto login, or manually login and skip the next cell. Add your username and password here. This account should be a real trusted account, to be used for pulling links from a application-acceptance based LinkedIN group. If you want to use a all-accepting linkedin group then just use a bot account for this part too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "trusted_password = os.environ.get('ACTUAL_LINKED_IN_PASS')\n",
    "trusted_username = os.environ.get('ACTUAL_LINKED_IN_ACC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_linkedin(driver, trusted_username, trusted_password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the opened tab to navigate to group page or 'my network' page collect_links will gather profile urls up to a chosen limit, checking against list of profiles already scraped. You could probably set this amount pretty high, like 1,000, and maybe not get in trouble since all that will happen from linkedIn's perspective is a login followed by scrolling down a huge list in a group. This step requires the driver to be in focus, possibly because it is executing js commands to scroll to top and to bottom. This doesn't seem like a huge issue in general since the links gathered in this step take 20x longer to actually scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../reference/scraped_profiles.txt', 'r') as file:\n",
    "    scraped_profiles = file.read().split('\\n')\n",
    "\n",
    "collected_links = collect_search_links(driver,scraped_profiles,target_count = 30)\n",
    "\n",
    "with open('../reference/links_to_scrape.txt', 'a') as file:\n",
    "    for link in collected_links:\n",
    "        file.write(\"%s\\n\" % link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only quit here if you are going to a different account for the profile scraping phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with a list of profiles accounts and user-agent strings, pick a random account and do some scraping. In a fully fledged version this would iterate through all of them rather than pick a random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "accounts_df = pd.read_csv('../reference/accounts.csv')\n",
    "\n",
    "# Filter the DataFrame to include only active accounts\n",
    "active_accounts = accounts_df[accounts_df['acc_status'] == 1]\n",
    "\n",
    "# Select a random row from the DataFrame\n",
    "random_account = active_accounts.sample(1).iloc[0]\n",
    "\n",
    "bot_username = random_account['username']\n",
    "user_agent = random_account['user_agent']\n",
    "bot_password = random_account['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = create_driver(user_agent)\n",
    "login_linkedin(driver, bot_username, bot_password)\n",
    "restriction_text = '</h1><p>We\\'ve restricted your account until '\n",
    "if restriction_text in driver.page_source:\n",
    "    print(driver.page_source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will work if the bot accounts are created with protonmail, using the same password as their associated linkedin acc. At some point I should probably generate a random list of human sounding emails and passwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Let's do a quick verification\" in driver.page_source:\n",
    "    grab_verification(driver, bot_username, bot_password)\n",
    "    # Get the verification code from the most recent pgp*.txt file\n",
    "elif \"Let's do a quick security check\" in driver.page_source: #captcha page\n",
    "    driver.switch_to.window(driver.current_window_handle) #brings page to focus for you\n",
    "    sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a number in scrape all to limit number of links scraped or leave blank to scrape all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../reference/scraped_profiles.txt', 'r') as file:\n",
    "    scraped_profiles = file.read().split('\\n')\n",
    "\n",
    "with open('../reference/links_to_scrape.txt', 'r') as file:\n",
    "    links_to_scrape = file.read().split('\\n')\n",
    "    \n",
    "links_to_scrape = list(set(links_to_scrape)) # Remove duplicates by converting to set and back to list\n",
    "\n",
    "# Remove any links that are already in scraped_profiles\n",
    "links_to_scrape = [link for link in links_to_scrape if link not in scraped_profiles]\n",
    "\n",
    "print(len(links_to_scrape))\n",
    "\n",
    "with open('../reference/links_to_scrape.txt', 'w') as file:\n",
    "    for link in links_to_scrape:\n",
    "        file.write(\"%s\\n\" % link)\n",
    "\n",
    "with open('../reference/links_to_scrape.txt', 'r') as file:\n",
    "    collected_links = file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_all(driver, collected_links,200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "craigslist-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
